{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Documents\\GitHub\\captum_influence\\.env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers.pipelines import TextClassificationPipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "\n",
    "import sys\n",
    "sys.path.append('c:\\\\Users\\\\alexg\\\\Documents\\\\GitHub\\\\captum_influence\\\\captum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concept import TCAV\n",
    "from concept import Concept\n",
    "from concept._utils.common import concepts_to_str\n",
    "import jsonlines\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextDataset(IterableDataset):\n",
    "        def __init__(self, filename, tokenizer, device, max_len = 32):\n",
    "            self.df = pd.read_csv(filename)\n",
    "            self.device = device\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_len = max_len\n",
    "            \n",
    "        def __iter__(self):\n",
    "            for _, row in self.df.iterrows():\n",
    "                tokenized_text = self.tokenizer(row['text'], padding='max_length', truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "                text = self.tokenizer.decode(tokenized_text['input_ids'][0], skip_special_tokens=True)\n",
    "\n",
    "                yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_float(f):\n",
    "        return float('{:.3f}'.format(f) if abs(f) >= 0.0005 else '{:.3e}'.format(f))\n",
    "\n",
    "def assemble_concept(name, id, tokenizer, device, concepts_path=\"data/tcav/text-sensitivity\"):\n",
    "        dataset = CustomTextDataset(concepts_path, tokenizer, device)\n",
    "        concept_iter = DataLoader(dataset, batch_size=1)\n",
    "        return Concept(id=id, name=name, data_iter=concept_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "from transformers.pipelines.base import GenericTensor\n",
    "\n",
    "\n",
    "class CaptumPipeline(TextClassificationPipeline):\n",
    "    def postprocess(self, model_outputs, function_to_apply=None, top_k=1, _legacy=True):\n",
    "        # `_legacy` is used to determine if we're running the naked pipeline and in backward\n",
    "        # compatibility mode, or if running the pipeline with `pipeline(..., top_k=1)` we're running\n",
    "        # the more natural result containing the list.\n",
    "        # Default value before `set_parameters`\n",
    "\n",
    "        outputs = model_outputs[\"logits\"][0]\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def preprocess(self, inputs, **tokenizer_kwargs) -> Dict[str, GenericTensor]:\n",
    "        return_tensors = self.framework\n",
    "        if isinstance(inputs, dict):\n",
    "            return self.tokenizer(**inputs, return_tensors=return_tensors, **tokenizer_kwargs)\n",
    "        elif isinstance(inputs, list) and len(inputs) == 1 and isinstance(inputs[0], list) and len(inputs[0]) == 2:\n",
    "            # It used to be valid to use a list of list of list for text pairs, keeping this path for BC\n",
    "            return self.tokenizer(\n",
    "                text=inputs[0][0], text_pair=inputs[0][1], return_tensors=return_tensors, **tokenizer_kwargs\n",
    "            )\n",
    "        elif isinstance(inputs, list):\n",
    "            # This is likely an invalid usage of the pipeline attempting to pass text pairs.\n",
    "            raise ValueError(\n",
    "                \"The pipeline received invalid inputs, if you are trying to send text pairs, you can try to send a\"\n",
    "                ' dictionary `{\"text\": \"My text\", \"text_pair\": \"My pair\"}` in order to send a text pair.'\n",
    "            )\n",
    "        return self.tokenizer(inputs, return_tensors=return_tensors, **tokenizer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCAVTransformerPipeline():\n",
    "    \"Wrapper for Captum TCAV framework usage with Huggingface Pipeline\"\n",
    "\n",
    "    def __init__(self, name:str, pipeline: CaptumPipeline, device: str):\n",
    "        self.__name = name\n",
    "        self.__pipeline = pipeline\n",
    "        self.__device = device\n",
    "        \n",
    "\n",
    "    def forward_func(self, inputs: tensor, position = 0):\n",
    "        \"\"\"\n",
    "            Wrapper around prediction method of pipeline\n",
    "        \"\"\"\n",
    "        return self.__pipeline.model(inputs)[0]\n",
    "    \n",
    "    def apply_concept(self, text: str, concept_sets: list, out_file: str):\n",
    "        \"\"\"\n",
    "            Main entry method. Passes text through series of transformations and through the model. \n",
    "            Calls visualization method.\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = self.generate_inputs(text)\n",
    "\n",
    "        prediction = self.__pipeline.model(inputs)\n",
    "        \n",
    "        # Tried initializing with model and forward_func, forward func doesn't work because direct access to model\n",
    "        #layers is required. Model does not return logits.\n",
    "        tcav = TCAV(self.__pipeline, layers=['model.deberta.encoder.layer.0'])\n",
    "\n",
    "        t = torch.argmax(prediction[0])\n",
    "\n",
    "        # interpret expects logits as outputs, but hf models do not want to output logits directly\n",
    "        positive_interpretations = tcav.interpret(\n",
    "                                            text,\n",
    "                                            experimental_sets=concept_sets,\n",
    "                                            target=t\n",
    "                                            )\n",
    "        \n",
    "        self.plot_tcav_scores(concept_sets, positive_interpretations, out_file=out_file, layers = ['model.deberta.encoder.layer'])\n",
    "\n",
    "\n",
    "    def plot_tcav_scores(experimental_sets, tcav_scores, out_file, layers = ['convs.2'], score_type='sign_count'):\n",
    "        fig, ax = plt.subplots(1, len(experimental_sets), figsize = (25, 7))\n",
    "\n",
    "        barWidth = 1 / (len(experimental_sets[0]) + 1)\n",
    "\n",
    "        for idx_es, concepts in enumerate(experimental_sets):\n",
    "            concepts = experimental_sets[idx_es]\n",
    "            concepts_key = concepts_to_str(concepts)\n",
    "            \n",
    "            layers = tcav_scores[concepts_key].keys()\n",
    "            pos = [np.arange(len(layers))]\n",
    "            for i in range(1, len(concepts)):\n",
    "                pos.append([(x + barWidth) for x in pos[i-1]])\n",
    "            _ax = (ax[idx_es] if len(experimental_sets) > 1 else ax)\n",
    "            for i in range(len(concepts)):\n",
    "                val = [format_float(scores[score_type][i]) for layer, scores in tcav_scores[concepts_key].items()]\n",
    "                _ax.bar(pos[i], val, width=barWidth, edgecolor='white', label=concepts[i].name)\n",
    "\n",
    "            # Add xticks on the middle of the group bars\n",
    "            _ax.set_xlabel('Set {}'.format(str(idx_es)), fontweight='bold', fontsize=16)\n",
    "            _ax.set_xticks([r + 0.3 * barWidth for r in range(len(layers))])\n",
    "            _ax.set_xticklabels(layers, fontsize=16)\n",
    "\n",
    "            # Create legend & Show graphic\n",
    "            _ax.legend(fontsize=16)\n",
    "        plt.savefig(out_file)\n",
    "        \n",
    "\n",
    "    def generate_inputs(self, text: str) -> tensor:\n",
    "        \"\"\"\n",
    "            Convenience method for generation of input ids as list of torch tensors\n",
    "        \"\"\"\n",
    "        return torch.tensor(self.__pipeline.tokenizer.encode(text, add_special_tokens=False), device = self.__device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(AutoModelForSequenceClassification):\n",
    "    def forward(self, *args, **kwargs):\n",
    "        outputs = super().forward(*args, **kwargs)\n",
    "        return outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--analsis_dir', default='out', type=str, help='Directory where attribution figures will be saved')\n",
    "# parser.add_argument('--concept_dir', default='data', type=str, help='Directory where concept files are stored')\n",
    "# parser.add_argument('--model_checkpoint', type=str, default='microsoft/deberta-v3-base', help='model checkpoint')\n",
    "# parser.add_argument('--a1_analysis_file', type=str, default='out/a1_analysis_data.jsonl', help='path to a1 analysis file')\n",
    "# parser.add_argument('--num_labels', default=2, type=int, help='Task number of labels')\n",
    "# parser.add_argument('--output_dir', default='out', type=str, help='Directory where model checkpoints will be saved')    \n",
    "# args = parser.parse_args()\n",
    "\n",
    "analysis_dir = os.getcwd() + '/out/attributions/'\n",
    "concept_dir = os.getcwd() + '/data/tcav/text-sensitivity/'\n",
    "model_checkpoint = 'microsoft/deberta-v3-base'\n",
    "a1_analysis_file = os.getcwd() + '/out/a1_analysis_data.jsonl'\n",
    "num_labels = 2\n",
    "output_dir = os.getcwd() + '/out/checkpoints/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Documents\\GitHub\\captum_influence\\.env\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'classifier.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#creating concepts, this code is functional\n",
    "neutral_concept = assemble_concept('neutral', 0, concepts_path=concept_dir + '/neutral_samples.csv', tokenizer=tokenizer, device=device)\n",
    "positive_concept = assemble_concept('positive', 1, concepts_path=concept_dir +'/positive_samples.csv', tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0-11): 12 x DebertaV2Layer(\n",
      "    (attention): DebertaV2Attention(\n",
      "      (self): DisentangledSelfAttention(\n",
      "        (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (pos_dropout): StableDropout()\n",
      "        (dropout): StableDropout()\n",
      "      )\n",
      "      (output): DebertaV2SelfOutput(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "        (dropout): StableDropout()\n",
      "      )\n",
      "    )\n",
      "    (intermediate): DebertaV2Intermediate(\n",
      "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (intermediate_act_fn): GELUActivation()\n",
      "    )\n",
      "    (output): DebertaV2Output(\n",
      "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "concepts = [[positive_concept, neutral_concept]]\n",
    "\n",
    "clf = transformers.pipeline(task=\"text-classification\",\n",
    "                            model=model,\n",
    "                            tokenizer=tokenizer,\n",
    "                            device=device,\n",
    "                            pipeline_class=CaptumPipeline,\n",
    "                            truncation=True,\n",
    "                            max_length=32,\n",
    "                            padding='max_length'\n",
    "                        )\n",
    "\n",
    "print(clf.model.deberta.encoder.layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.2417, 0.0108])]\n"
     ]
    }
   ],
   "source": [
    "print(clf('I love you'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Documents\\GitHub\\captum_influence\\.env\\Lib\\site-packages\\captum\\concept\\_utils\\classifier.py:130: UserWarning: Using default classifier for TCAV which keeps input both train and test datasets in the memory. Consider defining your own classifier that doesn't rely heavily on memory, for large number of concepts, by extending `Classifer` abstract class\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexg\\Documents\\GitHub\\captum_influence\\.env\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Concept(1, 'positive'), Concept(0, 'neutral')]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "indices should be either on cpu or on the same device as the indexed tensor (cpu)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexg\\Documents\\GitHub\\captum_influence\\tutorials\\TCAV_HF.ipynb Cell 13\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/captum_influence/tutorials/TCAV_HF.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m jsonlines\u001b[39m.\u001b[39mopen(a1_analysis_file, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m reader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/captum_influence/tutorials/TCAV_HF.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m reader:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/captum_influence/tutorials/TCAV_HF.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         tcav_model\u001b[39m.\u001b[39;49mapply_concept(obj[\u001b[39m\"\u001b[39;49m\u001b[39mreview\u001b[39;49m\u001b[39m\"\u001b[39;49m], concepts, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(output_dir, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexample_\u001b[39;49m\u001b[39m{\u001b[39;49;00midx\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m))\n",
      "\u001b[1;32mc:\\Users\\alexg\\Documents\\GitHub\\captum_influence\\tutorials\\TCAV_HF.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/captum_influence/tutorials/TCAV_HF.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(prediction[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/captum_influence/tutorials/TCAV_HF.ipynb#X13sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# interpret expects logits as outputs, but hf models do not want to output logits directly\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/captum_influence/tutorials/TCAV_HF.ipynb#X13sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m positive_interpretations \u001b[39m=\u001b[39m tcav\u001b[39m.\u001b[39;49minterpret(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/captum_influence/tutorials/TCAV_HF.ipynb#X13sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m                                     text,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/captum_influence/tutorials/TCAV_HF.ipynb#X13sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m                                     experimental_sets\u001b[39m=\u001b[39;49mconcept_sets,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/captum_influence/tutorials/TCAV_HF.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                                     target\u001b[39m=\u001b[39;49mt\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/captum_influence/tutorials/TCAV_HF.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m                                     )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexg/Documents/GitHub/captum_influence/tutorials/TCAV_HF.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplot_tcav_scores(concept_sets, positive_interpretations, out_file\u001b[39m=\u001b[39mout_file, layers \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mmodel.deberta.encoder.layer\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\alexg\\Documents\\GitHub\\captum_influence\\.env\\Lib\\site-packages\\captum\\log\\__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\alexg\\Documents\\GitHub\\captum_influence\\.env\\Lib\\site-packages\\captum\\concept\\_core\\tcav.py:663\u001b[0m, in \u001b[0;36mTCAV.interpret\u001b[1;34m(self, inputs, experimental_sets, target, additional_forward_args, processes, **kwargs)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[39mThis method computes magnitude and sign-based TCAV scores for each\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[39mexperimental sets in `experimental_sets` list.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    656\u001b[0m \n\u001b[0;32m    657\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    658\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mattribute_to_layer_input\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m kwargs, (\n\u001b[0;32m    659\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mPlease, set `attribute_to_layer_input` flag as a constructor \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    660\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39margument to TCAV class. In that case it will be applied \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    661\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mconsistently to both layer activation and layer attribution methods.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    662\u001b[0m )\n\u001b[1;32m--> 663\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_cavs(experimental_sets, processes\u001b[39m=\u001b[39;49mprocesses)\n\u001b[0;32m    665\u001b[0m scores: Dict[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Dict[\u001b[39mstr\u001b[39m, Tensor]]] \u001b[39m=\u001b[39m defaultdict(\n\u001b[0;32m    666\u001b[0m     \u001b[39mlambda\u001b[39;00m: defaultdict()\n\u001b[0;32m    667\u001b[0m )\n\u001b[0;32m    669\u001b[0m \u001b[39m# Retrieves the lengths of the experimental sets so that we can sort\u001b[39;00m\n\u001b[0;32m    670\u001b[0m \u001b[39m# them by the length and compute TCAV scores in batches.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexg\\Documents\\GitHub\\captum_influence\\.env\\Lib\\site-packages\\captum\\concept\\_core\\tcav.py:547\u001b[0m, in \u001b[0;36mTCAV.compute_cavs\u001b[1;34m(self, experimental_sets, force_train, processes)\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[39mfor\u001b[39;00m concepts \u001b[39min\u001b[39;00m experimental_sets:\n\u001b[0;32m    545\u001b[0m         \u001b[39mprint\u001b[39m(concepts)\n\u001b[0;32m    546\u001b[0m         cavs_list\u001b[39m.\u001b[39mappend(\n\u001b[1;32m--> 547\u001b[0m             train_cav(\n\u001b[0;32m    548\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_id,\n\u001b[0;32m    549\u001b[0m                 concepts,\n\u001b[0;32m    550\u001b[0m                 concept_key_to_layers[concepts_to_str(concepts)],\n\u001b[0;32m    551\u001b[0m                 cast(Classifier, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier),\n\u001b[0;32m    552\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_path,\n\u001b[0;32m    553\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclassifier_kwargs,\n\u001b[0;32m    554\u001b[0m             )\n\u001b[0;32m    555\u001b[0m         )\n\u001b[0;32m    557\u001b[0m \u001b[39m# list[Dict[concept, Dict[layer, list]]] => Dict[concept, Dict[layer, list]]\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[39mfor\u001b[39;00m cavs \u001b[39min\u001b[39;00m cavs_list:\n",
      "File \u001b[1;32mc:\\Users\\alexg\\Documents\\GitHub\\captum_influence\\.env\\Lib\\site-packages\\captum\\concept\\_core\\tcav.py:170\u001b[0m, in \u001b[0;36mtrain_cav\u001b[1;34m(model_id, concepts, layers, classifier, save_path, classifier_kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(inputs), torch\u001b[39m.\u001b[39mcat(labels)\n\u001b[0;32m    168\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(labelled_dataset, collate_fn\u001b[39m=\u001b[39mbatch_collate)\n\u001b[1;32m--> 170\u001b[0m classifier_stats_dict \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mtrain_and_eval(\n\u001b[0;32m    171\u001b[0m     dataloader, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mclassifier_kwargs\n\u001b[0;32m    172\u001b[0m )\n\u001b[0;32m    173\u001b[0m classifier_stats_dict \u001b[39m=\u001b[39m (\n\u001b[0;32m    174\u001b[0m     {} \u001b[39mif\u001b[39;00m classifier_stats_dict \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m classifier_stats_dict\n\u001b[0;32m    175\u001b[0m )\n\u001b[0;32m    177\u001b[0m weights \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39mweights()\n",
      "File \u001b[1;32mc:\\Users\\alexg\\Documents\\GitHub\\captum_influence\\.env\\Lib\\site-packages\\captum\\concept\\_utils\\classifier.py:181\u001b[0m, in \u001b[0;36mDefaultClassifier.train_and_eval\u001b[1;34m(self, dataloader, test_split_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm\u001b[39m.\u001b[39mfit(DataLoader(TensorDataset(x_train, y_train)))\n\u001b[0;32m    179\u001b[0m predict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm(x_test)\n\u001b[1;32m--> 181\u001b[0m predict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlm\u001b[39m.\u001b[39;49mclasses()[torch\u001b[39m.\u001b[39;49margmax(predict, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)]  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m    182\u001b[0m score \u001b[39m=\u001b[39m predict\u001b[39m.\u001b[39mlong() \u001b[39m==\u001b[39m y_test\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m    184\u001b[0m accs \u001b[39m=\u001b[39m score\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mmean()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: indices should be either on cpu or on the same device as the indexed tensor (cpu)"
     ]
    }
   ],
   "source": [
    "tcav_model = TCAVTransformerPipeline(name='tcav', pipeline=clf, device=device)\n",
    "\n",
    "idx = 0\n",
    "with jsonlines.open(a1_analysis_file, 'r') as reader:\n",
    "    for obj in reader:\n",
    "        tcav_model.apply_concept(obj[\"review\"], concepts, os.path.join(output_dir, f'example_{idx}'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "captum_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
